{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading language model from data/6-gram-wiki-char.lm.bz2...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from ngram import *\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# ngram model for scoring hypotheses\n",
    "ngram_model = LM(\"data/6-gram-wiki-char.lm.bz2\", n=6, verbose=False)\n",
    "\n",
    "def read_file(filename):\n",
    "    if filename[-4:] == \".bz2\":\n",
    "        with bz2.open(filename, 'rt') as f:\n",
    "            content = f.read()\n",
    "            f.close()\n",
    "    else:\n",
    "        with open(filename, 'r') as f:\n",
    "            content = f.read()\n",
    "            f.close()\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beam search algorithm\n",
    "def beam_search(plaintext_alph, cipher_text, ext_order, ext_limits=1, beam_size=1):\n",
    "    # partial hypotheses\n",
    "    scored_hypotheses = [(0, [])]\n",
    "    hypothesis_extensions = []\n",
    "\n",
    "    for cipher_sym in ext_order:\n",
    "        for hyp in scored_hypotheses:\n",
    "            for pl_sym in plaintext_alph:\n",
    "                new_hypothesis = hyp[1] + [(pl_sym, cipher_sym)]\n",
    "\n",
    "                if within_ext_limits(ext_limits, new_hypothesis):\n",
    "                    hypothesis_extensions.append((score(new_hypothesis, cipher_text), new_hypothesis))\n",
    "\n",
    "            if hypothesis_extensions:\n",
    "                hypothesis_extensions = histogram_prune(hypothesis_extensions, beam_size)\n",
    "                scored_hypotheses = [h for h in hypothesis_extensions]\n",
    "\n",
    "        hypothesis_extensions.clear()\n",
    "    return winning_hypothesis(scored_hypotheses)\n",
    "\n",
    "\n",
    "# If the hypothesis exceeds the constraint on many-to-one mapping\n",
    "def within_ext_limits(limit, hyp):\n",
    "    plaintxt_sym_counter = Counter([tup[0] for tup in hyp])\n",
    "    return not any(plaintxt_sym_counter[k] > limit[k] for k in plaintxt_sym_counter)\n",
    "\n",
    "\n",
    "# Hypothesis scoring function\n",
    "def score(hypothesis, text):\n",
    "    decipherment = ''.join([g_funct(hypothesis, ch) for ch in text])\n",
    "    bitstring = ''\n",
    "    for ch in decipherment:\n",
    "        if ch == '_':\n",
    "            bitstring += '.'\n",
    "        else:\n",
    "            bitstring += 'o'\n",
    "    return ngram_model.score_bitstring(decipherment, bitstring)\n",
    "\n",
    "\n",
    "# Returns plaintext string for a cipher symbol given a hypothesized mapping\n",
    "def g_funct(hypothesis, cipher_sym):\n",
    "    for tup in hypothesis:\n",
    "        if tup[1] == cipher_sym:\n",
    "            return tup[0]\n",
    "    return '_'\n",
    "\n",
    "\n",
    "# Returns a sublist of n best scoring hypotheses\n",
    "def histogram_prune(hypotheses, n=1):\n",
    "    hypotheses.sort(reverse=True)\n",
    "    return hypotheses[:n]\n",
    "\n",
    "\n",
    "# Returns the best hypothesis\n",
    "def winning_hypothesis(hypotheses):\n",
    "    return histogram_prune(hypotheses, 1)[0][1]\n",
    "\n",
    "\n",
    "# Get an extension order based on contiguous deciphered ngrams\n",
    "def get_ext_order(alphabet, cipher):\n",
    "    ext_order = [alphabet[0]]\n",
    "    alphabet.pop(0)\n",
    "\n",
    "    while alphabet:\n",
    "        max_sum = weighted_sum(alphabet[0], cipher, ext_order)\n",
    "        max_char = alphabet[0]\n",
    "\n",
    "        for a in alphabet[1:]:\n",
    "            curr_sum = weighted_sum(a, cipher, ext_order)\n",
    "\n",
    "            if curr_sum > max_sum:\n",
    "                max_sum = curr_sum\n",
    "                max_char = a\n",
    "\n",
    "        ext_order.append(max_char)\n",
    "        alphabet.remove(max_char)\n",
    "\n",
    "    return ext_order\n",
    "\n",
    "\n",
    "# Calculate the weighted sum for ext order candidates\n",
    "def weighted_sum(ch, cipher, ext_order):\n",
    "    sum = 0\n",
    "    for n in range(2, 7):\n",
    "        grams = [g for g in ngrams(cipher, n) if all(c in ext_order + [ch] for c in g)]\n",
    "        sum += len(grams) * n\n",
    "\n",
    "    return sum\n",
    "\n",
    "\n",
    "def read_gold(gold_file):\n",
    "    with open(gold_file) as f:\n",
    "        gold = f.read()\n",
    "    f.close()\n",
    "    gold = list(gold.strip())\n",
    "    return gold\n",
    "\n",
    "def symbol_error_rate(dec, _gold):\n",
    "    gold = read_gold(_gold)\n",
    "    correct = 0\n",
    "    if len(gold) == len(dec):\n",
    "        for (d,g) in zip(dec, gold):\n",
    "            if d==g:\n",
    "                correct += 1\n",
    "    wrong = len(gold)-correct\n",
    "    error = wrong/len(gold)\n",
    "    \n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Cipher and plaintext files\n",
    "    cipher = read_file(\"data/cipher.txt\").replace('\\n', '')\n",
    "    #cipher = read_file(\"sandhill_cranes.txt\").replace('\\n', '')\n",
    "    plaintxt = read_file(\"data/default.wiki.txt.bz2\")\n",
    "    \n",
    "    \n",
    "    # Cipher and plaintext alphabets\n",
    "    cipher_count = Counter([ch for ch in cipher if not ch == '\\n'])\n",
    "    #Order cipher letters to maximize contiguous ngrams for each partial hypothesis\n",
    "    ext_order = [tup[0] for tup in cipher_count.most_common()]\n",
    "    ext_order = get_ext_order(ext_order, cipher)\n",
    "    # English alphabet ordered by letter frequency\n",
    "    eng_alphabet = [ch for ch in 'etaoinshrdlcumwfgypbvkjxqz']\n",
    "    \n",
    "    # Extension limits for each plaintext letter\n",
    "    ext_limit = {'e': 4, 't': 3, 'a': 3, 'o': 3, 'i': 3, 'n': 2, 's': 2, 'h': 2,'r': 2, 'd': 2, \n",
    "                 'l': 2, 'c': 2, 'u': 2, 'm': 2, 'w': 1, 'f': 1, 'g': 1, 'y': 1, 'p': 1, 'b': 1, \n",
    "                 'v': 1, 'k': 1, 'j': 1, 'x': 1, 'q': 1, 'z': 1}\n",
    "    \n",
    "    ext_limit_2 = {'e':8 , 't': 8, 'a': 8, 'o': 6, 'i': 6, 'n': 6, 's': 6, 'h': 6,'r': 6, 'd':4, \n",
    "                 'l': 4, 'c': 4, 'u': 4, 'm': 4, 'w': 4, 'f': 4, 'g': 4, 'y': 4, 'p': 4, 'b': 4, \n",
    "                 'v': 2, 'k': 2, 'j': 2, 'x': 2, 'q': 2, 'z': 2}\n",
    "    \n",
    "    ext_limit_3 = {'e': 4, 't': 4, 'a': 3, 'o': 3, 'i': 3, 'n': 3, 's': 3, 'h': 3,'r': 3, 'd': 2,\n",
    "                  'l': 2, 'c': 2, 'u': 2, 'm': 2, 'w': 2, 'f': 2, 'g': 2, 'y': 2, 'p': 2, 'b': 2, \n",
    "                  'v': 2, 'k': 2, 'j': 2, 'x': 2, 'q': 2, 'z': 2}\n",
    "    \n",
    "    e = 6\n",
    "    ext_limit_4 = {'e': e, 't': e, 'a': e, 'o': e, 'i': e, 'n': e, 's': e, 'h': e,'r': e, 'd': e, \n",
    "                  'l': e, 'c': e, 'u': e, 'm': e, 'w': e, 'f': e, 'g': e, 'y': e, 'p': e, 'b': e,\n",
    "                  'v': e, 'k': e, 'j': e, 'x': e, 'q': e, 'z': e}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('e', '—'), ('e', '£'), ('e', 'À'), ('r', 'W'), ('t', 'I'), ('e', 'V'), ('n', '∞'), ('r', 'E'), ('t', 'X'), ('t', 'H'), ('h', 'M'), ('a', 'u'), ('a', '≈'), ('n', '–'), ('e', 'K'), ('h', '•'), ('a', 'º'), ('s', 'B'), ('s', '∑'), ('o', '“'), ('e', 'P'), ('h', 'A'), ('s', 'π'), ('a', 'F'), ('s', 'Ç'), ('t', 'y'), ('t', 'æ'), ('s', 'µ'), ('t', '+'), ('n', 'G'), ('r', '∏'), ('w', 'L'), ('i', 'J'), ('s', 'Z'), ('a', '√'), ('a', 'R'), ('d', 'O'), ('i', '∫'), ('o', 'T'), ('n', '‘'), ('c', '^'), ('c', 'Q'), ('r', 'D'), ('r', 'N'), ('u', 'Ã'), ('l', 'S'), ('i', 'ƒ'), ('r', '/'), ('o', '\\\\'), ('b', '¢'), ('i', '§'), ('g', 'Ω'), ('m', '∆'), ('c', 'j')]\n",
      "aiersrasiedahnthsretantanatesesetahitcategeonrctrselbrassedahaimauerecwhsiioonaterattstenraetenettsilbarnttrudtensoclsswiresssterthacaaeaseenterestswhoasseratchnorbarateasandertstnweucanttararotooiarsticheshtaaniwererethnotsiesatwhurhesbeianehasseroteondechloumaangriniithreelarraiismhessenatererastatsahasidosaaatritercleresanterrothaiswnrtsastaithriogwsheraissnasedatcainassionertittosecrnertoartnerthehate\n",
      "Error:  90.44117647058823 Accuracy:  9.558823529411764\n"
     ]
    }
   ],
   "source": [
    "    # Get the best decipherment hypothesis\n",
    "    best_hypothesis = beam_search(eng_alphabet, cipher, ext_order, ext_limit_4, 26)\n",
    "    # Decipher cipher text\n",
    "    decipherment = ''.join([g_funct(best_hypothesis, ch) for ch in cipher])\n",
    "\n",
    "    print(best_hypothesis)\n",
    "    print(decipherment)\n",
    "    \n",
    "    # gold decipherment\n",
    "    gold_file = \"data/_ref.txt\"\n",
    "    ser = symbol_error_rate(decipherment, gold_file)\n",
    "    print('Error: ', ser*100, 'Accuracy: ', (1-ser)*100)\n",
    "    \n",
    "    exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
