{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2: Decipherment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group **northernwolfpack**\n",
    "\n",
    "* Chithra Bhat, cbhat\n",
    "* Gustavo Felhberg, gfelhber\n",
    "* Heikal Badrulhisham, hbadrulh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model\n",
    "\n",
    "The baseline mode, using an ngram model and a beam search, was implemented by translating the pseudocode in the homework description into Python code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ngram Model (best results)\n",
    "\n",
    "This is an extension of the baseline method using a beam search and an ngram language model. The additions are:\n",
    "\n",
    "-Ordering the list of English alphabet letters by letter frequency. Initially the English alphabet referred to by the beam search was in the traditional order ('abcd...').\n",
    "\n",
    "-Obtaining the extension order of cipher symbols iteratively during the beam search, rather than obtaining a complete order prior. At each iteration of the beam search, the next cipher symbol for extension is chosen such that the next hypothesis expansion would result in a decipherment with the maximal contiguous ngrams.\n",
    "\n",
    "\n",
    "### Ideas tested\n",
    "The following are additions to the main algorithm that were implemented and tested, but not kept because they led to performance deterioration:\n",
    "\n",
    "* **constraints based on English orthography**: \n",
    "1) limits on the length of consecutive consonants or vowels. 2) insisting that any consonant or vowel sequences in a potential decipherment must also occur in the Wikipedia corpus. These constraints were tested as outright ban or gradient penalties (more constraint violations lower the score for a hypothesis in the beam search)\n",
    "* **variable extension limits**: Extension limits were changed from a fixed extension limit for all plaintext letters, to variable extension limits for each letter. More frequent English characters were given higher extension limits.\n",
    "\n",
    "### Main files:\n",
    "\n",
    "* **ngram_solution.ipynb**: working file for the ngram approach (not for final submission)\n",
    "\n",
    "* **orthography.py**: file for deriving constraints on English orthography\n",
    "\n",
    "* **ngram_solution_alternative.ipynb**: version of the model with a few differences in some methods, but with the same overall idea.\n",
    "\n",
    "* **sandhill_cranes.txt**: 1:1 substitution cipher text we used to test our models. The 'cipher' is capitalization of the plain text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLM Model\n",
    "\n",
    "We followed the approach described in the Thesis \"Decipherment of Substitution Ciphers with Neural Language Models\". To implement the sampling, we used the method nlm.py/next_chars(). However, this process was taking too long, so we tried to reduce the sampling in a way that, instead of sampling the whole sentence for every new hypothesis, we sampled only the minimun lenght of the sentence that covers all the mapped characters for the current new hypothesis. \n",
    "\n",
    "**Example:** Assume that there are tree mapped hypotheses. The mapped sentence would be: \".......................a.............v........c..a..va.....c.....\". Instead of sampling all the gaps for the whole sentence, we sampled until the first 'c' and scored the sentence up to the first character 'c'. Although it improved a little in terms of running time, we noted that the results lost some readability.\n",
    "\n",
    "The auxiliary methods for this model are the same used for the ngram model.\n",
    "\n",
    "After several trials with different combinations and considering the huge amount of time for each test (not using CPU), we decided to focus only in the Ngram solution.\n",
    "\n",
    "### Main files:\n",
    "\n",
    "* **nlm_solution.ipynb:** This file contains all the methods used to create the NLM model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading language model from data/6-gram-wiki-char.lm.bz2...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from ngram import *\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# ngram model for scoring hypotheses\n",
    "ngram_model = LM(\"data/6-gram-wiki-char.lm.bz2\", n=6, verbose=False)\n",
    "\n",
    "def read_file(filename):\n",
    "    if filename[-4:] == \".bz2\":\n",
    "        with bz2.open(filename, 'rt') as f:\n",
    "            content = f.read()\n",
    "            f.close()\n",
    "    else:\n",
    "        with open(filename, 'r') as f:\n",
    "            content = f.read()\n",
    "            f.close()\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beam search algorithm\n",
    "def beam_search(plaintext_alph, cipher_text, ext_order, ext_limits=1, beam_size=1):\n",
    "    # partial hypotheses\n",
    "    scored_hypotheses = [(0, [])]\n",
    "    hypothesis_extensions = []\n",
    "\n",
    "    for cipher_sym in ext_order:\n",
    "        for hyp in scored_hypotheses:\n",
    "            for pl_sym in plaintext_alph:\n",
    "                new_hypothesis = hyp[1] + [(pl_sym, cipher_sym)]\n",
    "\n",
    "                if within_ext_limits(ext_limits, new_hypothesis):\n",
    "                    hypothesis_extensions.append((score(new_hypothesis, cipher_text), new_hypothesis))\n",
    "\n",
    "            if hypothesis_extensions:\n",
    "                hypothesis_extensions = histogram_prune(hypothesis_extensions, beam_size)\n",
    "                scored_hypotheses = [h for h in hypothesis_extensions]\n",
    "\n",
    "        hypothesis_extensions.clear()\n",
    "    return winning_hypothesis(scored_hypotheses)\n",
    "\n",
    "\n",
    "# If the hypothesis exceeds the constraint on many-to-one mapping\n",
    "def within_ext_limits(limit, hyp):\n",
    "    plaintxt_sym_counter = Counter([tup[0] for tup in hyp])\n",
    "    return not any(plaintxt_sym_counter[k] > limit[k] for k in plaintxt_sym_counter)\n",
    "\n",
    "\n",
    "# Hypothesis scoring function\n",
    "def score(hypothesis, text):\n",
    "    decipherment = ''.join([g_funct(hypothesis, ch) for ch in text])\n",
    "    bitstring = ''.join(['o' if ch != '_' else '.' for ch in decipherment])\n",
    "    return ngram_model.score_bitstring(decipherment, bitstring)\n",
    "\n",
    "\n",
    "# Returns plaintext string for a cipher symbol given a hypothesized mapping\n",
    "def g_funct(hypothesis, cipher_sym):\n",
    "    for tup in hypothesis:\n",
    "        if tup[1] == cipher_sym:\n",
    "            return tup[0]\n",
    "    return '_'\n",
    "\n",
    "\n",
    "# Returns a sublist of n best scoring hypotheses\n",
    "def histogram_prune(hypotheses, n=1):\n",
    "    hypotheses.sort(reverse=True)\n",
    "    return hypotheses[:n]\n",
    "\n",
    "\n",
    "# Returns the best hypothesis\n",
    "def winning_hypothesis(hypotheses):\n",
    "    return histogram_prune(hypotheses, 1)[0][1]\n",
    "\n",
    "\n",
    "# Get an extension order based on contiguous deciphered ngrams\n",
    "def get_ext_order(alphabet, cipher):\n",
    "    ext_order = [alphabet[0]]\n",
    "    alphabet.pop(0)\n",
    "\n",
    "    while alphabet:\n",
    "        max_sum = weighted_sum(alphabet[0], cipher, ext_order)\n",
    "        max_char = alphabet[0]\n",
    "\n",
    "        for a in alphabet[1:]:\n",
    "            curr_sum = weighted_sum(a, cipher, ext_order)\n",
    "\n",
    "            if curr_sum > max_sum:\n",
    "                max_sum = curr_sum\n",
    "                max_char = a\n",
    "\n",
    "        ext_order.append(max_char)\n",
    "        alphabet.remove(max_char)\n",
    "\n",
    "    return ext_order\n",
    "\n",
    "\n",
    "# Calculate the weighted sum for ext order candidates\n",
    "def weighted_sum(ch, cipher, ext_order):\n",
    "    sum = 0\n",
    "    for n in range(2, 7):\n",
    "        grams = [g for g in ngrams(cipher, n) if all(c in ext_order + [ch] for c in g)]\n",
    "        sum += len(grams) * n\n",
    "\n",
    "    return sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cipher and plaintext files\n",
    "cipher = read_file(\"data/cipher.txt\").replace('\\n', '')\n",
    "plaintxt = read_file(\"data/default.wiki.txt.bz2\")\n",
    "\n",
    "# Cipher and plaintext alphabets\n",
    "cipher_count = Counter([ch for ch in cipher if not ch == '\\n'])\n",
    "# Order cipher letters to maximize contiguous ngrams for each partial hypothesis\n",
    "ext_order = [tup[0] for tup in cipher_count.most_common()]\n",
    "ext_order = get_ext_order(ext_order, cipher)\n",
    "# English alphabet ordered by letter frequency\n",
    "eng_alphabet = [ch for ch in 'etaoinshrdlcumwfgypbvkjxqz']\n",
    "\n",
    "# Variable extension limits for each plaintext letter (can be modified to simulate constant ext_limits\n",
    "# ext_limit = {'e': 4, 't': 3, 'a': 3, 'o': 3, 'i': 3, 'n': 2, 's': 2, 'h': 2,'r': 2, 'd': 2, \n",
    "#              'l': 2, 'c': 2, 'u': 2, 'm': 2, 'w': 1, 'f': 1, 'g': 1, 'y': 1, 'p': 1, 'b': 1, \n",
    "#              'v': 1, 'k': 1, 'j': 1, 'x': 1, 'q': 1, 'z': 1}\n",
    "\n",
    "e = 6\n",
    "ext_limit = {'e': e, 't': e, 'a': e, 'o': e, 'i': e, 'n': e, 's': e, 'h': e,'r': e, 'd': e, \n",
    "              'l': e, 'c': e, 'u': e, 'm': e, 'w': e, 'f': e, 'g': e, 'y': e, 'p': e, 'b': e,\n",
    "              'v': e, 'k': e, 'j': e, 'x': e, 'q': e, 'z': e}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('r', '—'), ('e', 'W'), ('e', '∏'), ('a', 'X'), ('s', '≈'), ('e', '–'), ('r', 'V'), ('e', 'B'), ('s', '•'), ('e', '+'), ('h', '\\\\'), ('h', '∑'), ('t', 'ƒ'), ('a', '∞'), ('s', 'F'), ('t', 'H'), ('a', 'Ç'), ('t', 'º'), ('t', '“'), ('e', 'E'), ('s', 'I'), ('t', 'y'), ('i', 'G'), ('s', 'K'), ('t', '£'), ('i', 'æ'), ('s', 'π'), ('r', 'P'), ('o', 'D'), ('n', 'R'), ('i', 'T'), ('a', 'A'), ('n', 'µ'), ('a', 'O'), ('n', 'Q'), ('a', '∫'), ('o', 'M'), ('n', 'J'), ('i', 'À'), ('i', 'u'), ('n', '√'), ('u', 'Z'), ('o', '/'), ('c', 'L'), ('n', '‘'), ('y', '^'), ('o', 'S'), ('o', 'N'), ('d', 'Ã'), ('r', '¢'), ('b', '§'), ('r', 'Ω'), ('r', '∆'), ('i', 'j')]\n"
     ]
    }
   ],
   "source": [
    "# Get the best decipherment hypothesis\n",
    "best_hypothesis = beam_search(eng_alphabet, cipher, ext_order, ext_limit, 26)\n",
    "# Decipher cipher text\n",
    "decipherment = ''.join([g_funct(best_hypothesis, ch) for ch in cipher])\n",
    "print(best_hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tarouoieaianseaseeresitsattrssartsontyisirrinontontorotheranaiarndreiycountthestresitseriotsstarainbornonstedairieinohectorhesaretoiynninusresteraincottheroneisahorseitisunearesnenctdynatstoneitthtsosannarnoininactorestsitsaninticodoaturrbieiathereherinarysotdrisarobiaasooitoneotaaurarheresarereshinestateaainnineettreyorerusitseeitatahcnetaseatbaaothrcasresthhasnranansainusninreinseterneareattesaretotsisi\n"
     ]
    }
   ],
   "source": [
    "print(decipherment)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
