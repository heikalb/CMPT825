{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Anoop Sarkar <anoop _at_ sfu.ca>\n",
    "# Do not distribute this code to anybody\n",
    "# without permission from the author\n",
    "\n",
    "# pylint: disable=C0301,C0111,C0103,C0325\n",
    "# pylint: disable=R1702,R0912,R0902,R0913,R0914,R0915\n",
    "\n",
    "import sys\n",
    "import math\n",
    "import random\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Unseen: provides lexical rules for unseen words\n",
    "#\n",
    "class Unseen:\n",
    "\n",
    "    # list of part of speech tags for unseen words\n",
    "\n",
    "    def __init__(self, filename):\n",
    "        self.postags = None\n",
    "        self.total = None\n",
    "        self.most_likely_tag = None\n",
    "        self.total = 0\n",
    "        self.postags = {}\n",
    "        for _line in open(filename, 'r'):\n",
    "            _line = _line[:-1]\n",
    "            (count, tag) = _line.split()\n",
    "            if tag in self.postags:\n",
    "                raise ValueError(\"each postag should occur exactly once\")\n",
    "            self.postags[tag] = int(count)\n",
    "            self.total += int(count)\n",
    "        self.compute_log_prob()\n",
    "\n",
    "    def compute_log_prob(self):\n",
    "        for tag in self.postags:\n",
    "            self.postags[tag] = math.log(self.postags[tag] / self.total, 2)\n",
    "\n",
    "    def tags_for_unseen(self):\n",
    "        for tag in self.postags:\n",
    "            yield (tag, self.postags[tag])\n",
    "\n",
    "    def get_most_likely_tag(self):\n",
    "        if self.most_likely_tag is None:\n",
    "            (self.most_likely_tag, _) = sorted(self.postags.items(),\n",
    "                                               key=itemgetter(1)).pop()\n",
    "        return self.most_likely_tag\n",
    "\n",
    "# end of class Unseen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Pcfg:\n",
    "# Implements a non-strict Chomsky Normal Form probabilitic context free grammar\n",
    "# can have rules of the form A -> B C, A -> B (hence non-strict), or A -> a\n",
    "# A, B, C are non-terminals, a are terminals\n",
    "#\n",
    "# Format of the file for a rule lhs -> left right with its count is:\n",
    "# count lhs left [right]\n",
    "# the log_prob for each rule is computed on demand\n",
    "#\n",
    "class Pcfg:\n",
    "\n",
    "    # read in the file containing the weighted context-free grammar\n",
    "    # the prob for each rule is computed on the fly based on the weights\n",
    "    # normalized by the lhs symbol as per the usual definition of PCFGs\n",
    "    def __init__(self, filelist, startsym='TOP', allowed_words_file='allowed_words.txt', verbose=0):\n",
    "        self.startsym = startsym\n",
    "        self.allowed_words = set(line.strip() for line in open(allowed_words_file))\n",
    "        self.verbose = verbose\n",
    "        self.last_rule = -1\n",
    "        # each rule is indexed by a number i, where\n",
    "        # rule[i] = (lhs, (left, right), count, log_prob)\n",
    "        self.rules = {}\n",
    "\n",
    "        # forward index from lhs to list of rule numbers\n",
    "        self.lhs_rules = {}\n",
    "\n",
    "        # reverse index from (left,right) to a rule index\n",
    "        self.rhs = {}\n",
    "\n",
    "        # total count over all rhs for each lhs in the pcfg\n",
    "        self.lhs_count = {}\n",
    "        self.lhs_total_count = 0\n",
    "\n",
    "        # special symbol to mark a unary rule A -> B which is written as A -> B <Unary>\n",
    "        self.unary = '<Unary>'\n",
    "\n",
    "        for filename in filelist:\n",
    "            print(\"#reading grammar file: {}\".format(filename), file=sys.stderr)\n",
    "            linenum = 0\n",
    "            for _line in open(filename, 'r'):\n",
    "                linenum += 1\n",
    "                if _line.find('#') != -1:\n",
    "                    _line = _line[:_line.find('#')] # strip comments\n",
    "                _line = _line.strip()\n",
    "                if _line == \"\":\n",
    "                    continue\n",
    "                f = _line.split()\n",
    "                if len(f) > 4:\n",
    "                    # only CNF rules allowed\n",
    "                    raise ValueError(\"Error: >2 symbols in right hand side at line %d: %s\"\n",
    "                                     % (linenum, ' '.join(f)))\n",
    "                if len(f) < 3:\n",
    "                    # empty rules not allowed\n",
    "                    raise ValueError(\"Error: unexpected line at line %d: %s\"\n",
    "                                     % (linenum, ' '.join(f)))\n",
    "                # count lhs left [right]\n",
    "                try:\n",
    "                    count = int(f[0])\n",
    "                except:\n",
    "                    raise ValueError(\"Rule must be COUNT LHS RHS. Found {}\".format(\" \".join(f)))\n",
    "                (count, lhs, left) = (count, f[1], f[2])\n",
    "                if len(f) < 4:\n",
    "                    right = self.unary\n",
    "                else:\n",
    "                    right = f[3]\n",
    "                if lhs == left and right == self.unary:\n",
    "                    print(\"#Ignored cycle {} -> {}\".format(lhs, left), file=sys.stderr)\n",
    "                    continue\n",
    "                self.last_rule += 1\n",
    "                self.rules[self.last_rule] = (lhs, (left, right), count, None)\n",
    "\n",
    "                if self.verbose > 1:\n",
    "                    print(\"Rule: {}\".format(self.rules[self.last_rule]), file=sys.stderr)\n",
    "\n",
    "                if lhs in self.lhs_rules:\n",
    "                    self.lhs_rules[lhs].append(self.last_rule)\n",
    "                else:\n",
    "                    self.lhs_rules[lhs] = [self.last_rule]\n",
    "\n",
    "                if (left, right) in self.rhs:\n",
    "                    self.rhs[left, right].append(self.last_rule)\n",
    "                else:\n",
    "                    self.rhs[left, right] = [self.last_rule]\n",
    "\n",
    "                if lhs in self.lhs_count:\n",
    "                    self.lhs_count[lhs] += count\n",
    "                else:\n",
    "                    self.lhs_count[lhs] = count\n",
    "                self.lhs_total_count += count\n",
    "\n",
    "    # computes the log_prob of a rule using the counts collected for each lhs\n",
    "    # it caches the value into the rules table after computing the probabiilty\n",
    "    # for each rule\n",
    "    def get_log_prob(self, rule_number):\n",
    "        if rule_number in self.rules:\n",
    "            (lhs, rhs, count, log_prob) = self.rules[rule_number]\n",
    "            if log_prob is not None:\n",
    "                return log_prob\n",
    "            log_prob = math.log(count / self.lhs_count[lhs], 2)\n",
    "            self.rules[rule_number] = (lhs, rhs, count, log_prob)\n",
    "        else:\n",
    "            raise ValueError(\"rule number %d not found\" % rule_number)\n",
    "        return log_prob\n",
    "\n",
    "    def get_rule(self, rule_number):\n",
    "        log_prob = self.get_log_prob(rule_number)\n",
    "        if log_prob is None:\n",
    "            raise ValueError(\"rule has no log_prob: {}\".format(self.rules[rule_number]))\n",
    "        return self.rules[rule_number]\n",
    "\n",
    "    def rule_iterator(self, left, right):\n",
    "        if (left, right) in self.rhs:\n",
    "            for rule_number in self.rhs[left, right]:\n",
    "                yield rule_number\n",
    "        else:\n",
    "            return\n",
    "\n",
    "    # returns the prior probability of a nonTerminal\n",
    "    def get_prior(self, lhs):\n",
    "        if lhs in self.lhs_count:\n",
    "            return math.log(self.lhs_count[lhs] / self.lhs_total_count, 2)\n",
    "        raise ValueError(\"%s: missing lhs\" % lhs)\n",
    "\n",
    "    def __str__(self):\n",
    "        output = \"\"\n",
    "        for _i in range(0, self.last_rule+1):\n",
    "            log_prob = self.get_log_prob(_i)\n",
    "            (lhs, (left, right), count, log_prob) = self.rules[_i]\n",
    "            output += \" \".join([lhs, left, right, str(count), str(log_prob), \"\\n\"])\n",
    "        for _i in self.lhs_count:\n",
    "            if self.verbose:\n",
    "                print(\"#Prior: {} {}\".format(_i, self.get_prior(_i)), file=sys.stderr)\n",
    "        return output\n",
    "\n",
    "# end of class Pcfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PcfgGenerator contains the functions that allow sampling\n",
    "# of derivations from a PCFG. The output can be either the strings\n",
    "# or the trees.\n",
    "#\n",
    "# There is a small chance that the generator function will not\n",
    "# terminate. To make sure this outcome is avoided we use a limit\n",
    "# on how unlikely the generated derivation should be. If during\n",
    "# generation we go below this limit on the probability we stop\n",
    "# and restart the generation process.\n",
    "class PcfgGenerator:\n",
    "\n",
    "    def __init__(self, _gram, verbose=0, limit=1e-300):\n",
    "        self.gram = None # PCFG to be used by the generator\n",
    "        self.verbose = verbose\n",
    "        self.restart_limit = None # can be set using the constructor\n",
    "        self.restart_limit = limit\n",
    "        self.gram = _gram\n",
    "        # num_samples is the number of sampled words from allowed_words\n",
    "        # if the grammar produces entirely invalid sentence\n",
    "        self.num_samples = 1\n",
    "        random.seed()\n",
    "\n",
    "    def flatten_tree(self, tree):\n",
    "        sentence = []\n",
    "        if isinstance(tree, tuple):\n",
    "            (_, left_tree, right_tree) = tree\n",
    "            for n in (self.flatten_tree(left_tree), self.flatten_tree(right_tree)):\n",
    "                sentence.extend(n)\n",
    "        else:\n",
    "            if tree is not self.gram.unary:\n",
    "                sentence = [tree]\n",
    "        return sentence\n",
    "\n",
    "    def check_allowed(self, sentence):\n",
    "        if not sentence:\n",
    "            print(\"ERROR: sampled sentence is empty\", file=sys.stderr)\n",
    "            return random.sample(self.gram.allowed_words, self.num_samples)\n",
    "        new_sentence = []\n",
    "        for w in sentence:\n",
    "            if w not in self.gram.allowed_words:\n",
    "                print(\"ERROR: word {} was sampled but is not allowed\".format(w), file=sys.stderr)\n",
    "                new_sentence.append(random.sample(self.gram.allowed_words, 1)[0])\n",
    "            else:\n",
    "                new_sentence.append(w)\n",
    "        #assert(len(new_sentence) == len(sentence))\n",
    "        return new_sentence\n",
    "\n",
    "    def generate(self, parsetree=False):\n",
    "        rule = self.gen_pick_one(self.gram.startsym)\n",
    "        print(rule)\n",
    "        if self.verbose:\n",
    "            print(\"#getrule: {}\".format(self.gram.get_rule(rule)), file=sys.stderr)\n",
    "        gen_tree = self.gen_from_rule(rule)\n",
    "        return gen_tree if parsetree else self.check_allowed(self.flatten_tree(gen_tree))\n",
    "\n",
    "    def gen_pick_one(self, lhs):\n",
    "        r = random.random()\n",
    "        if self.verbose:\n",
    "            print(\"#random number: {}\".format(r), file=sys.stderr)\n",
    "        output_log_prob = math.log(r, 2)\n",
    "        accumulator = 0.0\n",
    "        rule_picked = None\n",
    "        for r in self.gram.lhs_rules[lhs]:\n",
    "            if self.verbose:\n",
    "                print(\"#getrule: {}\".format(self.gram.get_rule(r)), file=sys.stderr)\n",
    "            log_prob = self.gram.get_log_prob(r)\n",
    "            # convert to prob from log_prob in order to add with accumulator\n",
    "            prob = math.pow(2, log_prob)\n",
    "            if output_log_prob < math.log(prob + accumulator, 2):\n",
    "                rule_picked = r\n",
    "                break\n",
    "            else:\n",
    "                accumulator += prob\n",
    "        if rule_picked is None:\n",
    "            raise ValueError(\"no rule found for %s\" % lhs)\n",
    "        if self.verbose:\n",
    "            print(\"#picked rule %d: %s\" % (rule_picked, self.gram.rules[rule_picked]), file=sys.stderr)\n",
    "            print(self.gram.rules[rule_picked])\n",
    "        return rule_picked\n",
    "\n",
    "    def get_yield(self, sym):\n",
    "        return sym if sym not in self.gram.lhs_rules else self.gen_from_rule(self.gen_pick_one(sym))\n",
    "\n",
    "    def gen_from_rule(self, rule_number):\n",
    "        (lhs, (left, right), _, _) = self.gram.rules[rule_number]\n",
    "        if self.verbose:\n",
    "            print(\"#%s -> %s %s\" % (lhs, left, right), file=sys.stderr)\n",
    "        left_tree = self.get_yield(left)\n",
    "        right_tree = self.gram.unary if right is self.gram.unary else self.get_yield(right)\n",
    "        return (lhs, left_tree, right_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CkyParse contains the main parsing routines\n",
    "# including routines for printing out the best tree and pruning\n",
    "#\n",
    "class CkyParse:\n",
    "\n",
    "    def __init__(self, _gram, verbose=0, use_prior=True, use_pruning=True, beamsize=0.0001, unseen_file=\"unseen.tags\"):\n",
    "        self.gram = _gram # PCFG to be used by the grammar\n",
    "        self.verbose = verbose\n",
    "        if unseen_file != \"\":\n",
    "            self.unseen = Unseen(unseen_file)\n",
    "        else:\n",
    "            self.unseen = None\n",
    "        self.use_prior = use_prior\n",
    "        self.use_pruning = use_pruning\n",
    "        self.beam = math.log(float(beamsize), 2)\n",
    "        self.chart = None # chart data structure to be used by the parser\n",
    "        self._NINF = float('1e-323') # 64 bit double underflows for math.log(1e-324)\n",
    "        self._LOG_NINF = math.log(self._NINF, 2)\n",
    "\n",
    "    def prune(self, i, j):\n",
    "        if not self.use_pruning:\n",
    "            return 0\n",
    "        num_pruned = 0\n",
    "        if (i, j) in self.chart:\n",
    "            tbl = self.chart[i, j]\n",
    "            max_log_prob = self._LOG_NINF\n",
    "            best_lhs = None\n",
    "\n",
    "            for lhs in tbl.keys():\n",
    "                (log_prob, back_pointer) = tbl[lhs]\n",
    "                max_log_prob = max(log_prob, max_log_prob)\n",
    "                if max_log_prob == log_prob:\n",
    "                    best_lhs = lhs\n",
    "\n",
    "            new_table = {}\n",
    "            if self.use_prior:\n",
    "                lowest = max_log_prob + self.beam + self.gram.get_prior(best_lhs)\n",
    "            else:\n",
    "                lowest = max_log_prob + self.beam\n",
    "            for lhs in tbl.keys():\n",
    "                (log_prob, back_pointer) = tbl[lhs]\n",
    "                save_log_prob = log_prob\n",
    "                if self.use_prior:\n",
    "                    log_prob += self.gram.get_prior(lhs)\n",
    "                if log_prob < lowest:\n",
    "                    if self.verbose:\n",
    "                        print(\"#pruning: {} {} {} {} {}\".format(i, j, lhs, log_prob, lowest),\n",
    "                              file=sys.stderr)\n",
    "                    num_pruned += 1\n",
    "                    continue\n",
    "                new_table[lhs] = (save_log_prob, back_pointer)\n",
    "            self.chart[i, j] = new_table\n",
    "        return num_pruned\n",
    "\n",
    "    def insert(self, i, j, lhs, log_prob, back_pointer):\n",
    "        if (i, j) in self.chart:\n",
    "            if lhs in self.chart[i, j]:\n",
    "                prev_log_prob = self.chart_get_log_prob(i, j, lhs)\n",
    "                if log_prob < prev_log_prob:\n",
    "                    return False\n",
    "        else:\n",
    "            self.chart[i, j] = {}\n",
    "        self.chart[i, j][lhs] = (log_prob, back_pointer)\n",
    "        if self.verbose:\n",
    "            print(\"#inserted: {} {} {} {}\".format(i, j, lhs, log_prob), file=sys.stderr)\n",
    "        return True\n",
    "\n",
    "    def handle_unary_rules(self, i, j):\n",
    "        # we have to allow for the fact that B -> C might lead\n",
    "        # to another rule A -> B for the same span\n",
    "        unary_list = [entry for entry in self.chart_entry(i, j)]\n",
    "        for rhs in unary_list:\n",
    "            rhs_log_prob = self.chart_get_log_prob(i, j, rhs)\n",
    "            for rule_number in self.gram.rule_iterator(rhs, self.gram.unary):\n",
    "                (lhs, _, _, log_prob) = self.gram.get_rule(rule_number)\n",
    "                # rhs == left\n",
    "                if lhs == rhs:\n",
    "                    raise ValueError(\"Found a cycle\", lhs, \"->\", rhs)\n",
    "                back_pointer = (-1, rhs, self.gram.unary)\n",
    "                if self.verbose:\n",
    "                    print(\"log_prob: {} rhs_log_prob: {}\".format(log_prob, rhs_log_prob), file=sys.stderr)\n",
    "                if self.insert(i, j, lhs, log_prob + rhs_log_prob, back_pointer):\n",
    "                    unary_list.append(lhs)\n",
    "\n",
    "    def chart_entry(self, i, j):\n",
    "        if (i, j) in self.chart:\n",
    "            for item in self.chart[i, j].keys():\n",
    "                yield item\n",
    "        else:\n",
    "            return\n",
    "\n",
    "    def chart_get_log_prob(self, i, j, lhs):\n",
    "        if (i, j) in self.chart:\n",
    "            # Each entry in the chart for i, j is a hash table with key lhs\n",
    "            # and value equals the tuple (log_prob, back_pointer)\n",
    "            # This function returns the first element of the tuple\n",
    "            return self.chart[i, j][lhs][0]\n",
    "        raise ValueError(\"Could not find {}, {} in chart\".format(i, j))\n",
    "\n",
    "    def parse(self, input_sent):\n",
    "        # chart has max size len(input_sent)*len(input_sent)\n",
    "        # each entry in the chart is a hashtable with\n",
    "        # key=lhs and value=(log_prob, back_pointer)\n",
    "        self.chart = {}\n",
    "        num_pruned = 0\n",
    "\n",
    "        # insert all rules of type NonTerminal -> terminal\n",
    "        # where terminal matches some word in the input_sent\n",
    "        for (i, word) in enumerate(input_sent):\n",
    "            j = i+1\n",
    "            if (word, self.gram.unary) in self.gram.rhs:\n",
    "                for rule_number in self.gram.rhs[(word, self.gram.unary)]:\n",
    "                    (lhs, _, _, log_prob) = self.gram.get_rule(rule_number)\n",
    "                    self.insert(i, j, lhs, log_prob, None)\n",
    "            else:\n",
    "                print(\"#using unseen part of speech for {}\".format(word), file=sys.stderr)\n",
    "                if self.unseen is None:\n",
    "                    raise ValueError(\"cannot find terminal symbol\", word)\n",
    "                else:\n",
    "                    for (tag, log_prob) in self.unseen.tags_for_unseen():\n",
    "                        self.insert(i, j, tag, log_prob, None)\n",
    "            self.handle_unary_rules(i, j)\n",
    "\n",
    "        # do not prune lexical rules\n",
    "        # recursively insert nonterminal lhs\n",
    "        # for rule lhs -> left right into chart[(i, j)]\n",
    "        # if left belongs to the chart for span i,k\n",
    "        # and right belongs to the chart for span k, j\n",
    "        N = len(input_sent)+1\n",
    "        for j in range(2, N):\n",
    "            for i in range(j-2, -1, -1):\n",
    "                # handle the case for the binary branching rules lhs -> left right\n",
    "                for k in range(i+1, j):\n",
    "                    # handle the unary rules lhs -> rhs\n",
    "                    for left in self.chart_entry(i, k):\n",
    "                        for right in self.chart_entry(k, j):\n",
    "                            left_log_prob = self.chart_get_log_prob(i, k, left)\n",
    "                            right_log_prob = self.chart_get_log_prob(k, j, right)\n",
    "                            for rule_number in self.gram.rule_iterator(left, right):\n",
    "                                (lhs, _, _, log_prob) = self.gram.get_rule(rule_number)\n",
    "                                back_pointer = (k, left, right)\n",
    "                                self.insert(i, j, lhs,\n",
    "                                            log_prob + left_log_prob + right_log_prob,\n",
    "                                            back_pointer)\n",
    "                # handle the unary rules lhs -> rhs\n",
    "                self.handle_unary_rules(i, j)\n",
    "                # prune each span\n",
    "                num_pruned += self.prune(i, j)\n",
    "        if self.verbose:\n",
    "            print(\"#number of items pruned: {}\".format(num_pruned), file=sys.stderr)\n",
    "\n",
    "        sent_log_prob = self._LOG_NINF\n",
    "        N = len(input_sent)\n",
    "        if (0, N) in self.chart:\n",
    "            if self.gram.startsym in self.chart[0, N]:\n",
    "                (sent_log_prob, back_pointer) = self.chart[0, N][self.gram.startsym]\n",
    "        if self.verbose:\n",
    "            print(\"#sentence log prob = {}\".format(sent_log_prob), file=sys.stderr)\n",
    "        return sent_log_prob\n",
    "\n",
    "    # default_tree provides a parse tree for input_sent w0,..,wN-1 when\n",
    "    # the parser is unable to find a valid parse (no start symbol in\n",
    "    # span 0, N). The default parse is simply the start symbol with\n",
    "    # N children:\n",
    "    # (TOP (P0 w0) (P1 w1) ... (PN-1 wN-1))\n",
    "    # where Pi is the most likely part of speech tag for that word\n",
    "    # from training data.\n",
    "    # If the word is unknown it receives the most likely tag from\n",
    "    # training (across all words).\n",
    "    # if the Unseen class does not return a tag default_tree uses\n",
    "    # a default part of speech tag X.\n",
    "    def default_tree(self, input_sent):\n",
    "        tag = \"X\" if self.unseen is None else self.unseen.get_most_likely_tag()\n",
    "        taggedInput = map(lambda z: \"(\" + tag + \" \" + z + \")\", input_sent)\n",
    "        return \"(\" + self.gram.startsym + \" \" + \" \".join(taggedInput) + \")\"\n",
    "\n",
    "    # best_tree returns the most likely parse\n",
    "    # if there was a parse there must be a start symbol S in span 0, N\n",
    "    # then the best parse looks like (S (A ...) (B ...)) for some\n",
    "    # A in span 0,k and B in span k,N; the function extract_best_tree\n",
    "    # recursively fills in the trees under the start symbol S\n",
    "    def best_tree(self, input_sent):\n",
    "        N = len(input_sent)\n",
    "        startsym = self.gram.startsym\n",
    "        if (0, N) in self.chart:\n",
    "            if startsym in self.chart[0, N]:\n",
    "                return self.extract_best_tree(input_sent, 0, N, startsym)\n",
    "        print(\"#No parses found for: {}\".format(\" \".join(input_sent)), file=sys.stderr)\n",
    "        return self.default_tree(input_sent)\n",
    "\n",
    "    # extract_best_tree uses back_pointers to recursively find the\n",
    "    # best parse top-down:\n",
    "    # for each span i, j and non-terminal A (sym below), the parsing\n",
    "    # algorithm has recorded the best path to that non-terminal A\n",
    "    # using the back_pointer (k, left_sym, right_sym) which means\n",
    "    # there is a rule A -> left_sym right_sym and that left_sym spans\n",
    "    # i,k and right_sym spans k, j. Recursively calling extract_best_tree\n",
    "    # on spans i,k,left_sym and k, j, right_sym will provide the necessary\n",
    "    # parts to fill in the dotted parts in the tree:\n",
    "    # (A (left_sym ...) (right_sym ...))\n",
    "    # the parser records k == -1 when it inserts a unary rule:\n",
    "    # A -> left_sym <Unary>\n",
    "    # so a single recursive call to extract_best_tree fills in the\n",
    "    # dotted parts of the tree:\n",
    "    # (A (left_sym ...))\n",
    "    def extract_best_tree(self, input_sent, i, j, sym):\n",
    "        if (i, j) in self.chart:\n",
    "            if sym in self.chart[i, j]:\n",
    "                (_, back_pointer) = self.chart[i, j][sym]\n",
    "                if back_pointer is None:\n",
    "                    return \"(\" + sym + \" \" + input_sent[i] + \")\"\n",
    "                (k, left_sym, right_sym) = back_pointer\n",
    "                if k == -1:\n",
    "                    # unary rule\n",
    "                    left_tree = self.extract_best_tree(input_sent, i, j, left_sym)\n",
    "                    right_tree = \"\"\n",
    "                else:\n",
    "                    # binary rule\n",
    "                    left_tree = self.extract_best_tree(input_sent, i, k, left_sym)\n",
    "                    right_tree = self.extract_best_tree(input_sent, k, j, right_sym)\n",
    "                return \"(\" + sym + \" \" + left_tree + \" \" + right_tree + \")\"\n",
    "        raise ValueError(\"cannot find span:\", i, j, sym)\n",
    "\n",
    "    def parse_sentences(self, sentences):\n",
    "        corpus_cross_entropy = self._LOG_NINF\n",
    "        corpus_len = 0\n",
    "        total_log_prob = None\n",
    "        parses = []\n",
    "        for sent in sentences:\n",
    "            sent = sent.strip()\n",
    "            input_sent = sent.split()\n",
    "            length = len(input_sent)\n",
    "            if length <= 0:\n",
    "                continue\n",
    "            if sent[0] == '#':\n",
    "                if self.verbose:\n",
    "                    print(\"#skipping comment line in input_sent: {}\".format(sent), file=sys.stderr)\n",
    "                continue\n",
    "            corpus_len += length\n",
    "            print(\"#parsing: {}\".format(input_sent), file=sys.stderr)\n",
    "            sent_log_prob = self.parse(input_sent)\n",
    "            total_log_prob = sent_log_prob if total_log_prob is None else total_log_prob + sent_log_prob\n",
    "            best_tree = self.best_tree(input_sent)\n",
    "            parses.append(best_tree)\n",
    "            print(best_tree)\n",
    "        if corpus_len:\n",
    "            corpus_cross_entropy = total_log_prob / corpus_len\n",
    "            print(\"#-cross entropy (bits/word): %g\" % corpus_cross_entropy, file=sys.stderr)\n",
    "        return (corpus_cross_entropy, parses)\n",
    "\n",
    "    def parse_file(self, filename):\n",
    "        parses = []\n",
    "        with open(filename, 'r') as fh:\n",
    "            parses = self.parse_stream(fh)\n",
    "        return parses\n",
    "\n",
    "    def parse_stream(self, handle):\n",
    "        if self.verbose:\n",
    "            print(\"parsing from stream: {}\".format(handle), file=sys.stderr)\n",
    "        sentences = []\n",
    "        for line in handle:\n",
    "            line = line.strip()\n",
    "            sentences.append(line)\n",
    "        parses = self.parse_sentences(sentences)\n",
    "        return parses\n",
    "\n",
    "# end of class CkyParse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "#     import argparse\n",
    "\n",
    "#     argparser = argparse.ArgumentParser()\n",
    "#     argparser.add_argument('-v', '--verbose', action='count', default=0,\n",
    "#                            help=\"verbose output\")\n",
    "#     argparser.add_argument(\"-s\", \"--startsymbol\", dest=\"startsym\", type=str, default=\"TOP\",\n",
    "#                            help=\"start symbol\")\n",
    "#     argparser.add_argument(\"-i\", \"--parse\", dest=\"parse_mode\", action=\"store_true\",\n",
    "#                            help=\"parsing mode; takes sentence and produces parse if possible\")\n",
    "#     argparser.add_argument(\"-o\", \"--generate\", dest=\"generate_mode\", action=\"store_true\",\n",
    "#                            help=\"generate mode; takes grammar and produces sentences if possible\")\n",
    "#     argparser.add_argument(\"-n\", \"--numsentences\", dest=\"num_sentences\", type=int, default=20,\n",
    "#                            help=\"number of sentences to generate; in --generate mode\")\n",
    "#     argparser.add_argument(\"-r\", \"--prior\", dest=\"use_prior\", action=\"store_false\",\n",
    "#                            help=\"use prior for pruning\")\n",
    "#     argparser.add_argument(\"-p\", \"--pruning\", dest=\"use_pruning\", action=\"store_false\",\n",
    "#                            help=\"use prior for pruning\")\n",
    "#     argparser.add_argument(\"-u\", \"--unseentags\", dest=\"unseen_file\", type=str, default=\"unseen.tags\",\n",
    "#                            help=\"use prior for pruning\")\n",
    "#     argparser.add_argument(\"-b\", \"--beam\", dest=\"beam\", type=float, default=0.0001,\n",
    "#                            help=\"use prior for pruning\")\n",
    "#     argparser.add_argument(\"-a\", \"--allowedwords\", dest=\"allowed_words_file\", type=str,\n",
    "#                            default=\"allowed_words.txt\",\n",
    "#                            help=\"only use this list of words when parsing and generating\")\n",
    "#     argparser.add_argument(\"-g\", \"--grammars\", nargs=argparse.ONE_OR_MORE, dest=\"grammar_files\",\n",
    "#                            type=str, default=[\"S1.gr\", \"S2.gr\", \"Vocab.gr\"],\n",
    "#                            help=\"list of grammar files; typically: S1.gr S2.gr Vocab.gr\")\n",
    "\n",
    "#     args = argparser.parse_args()\n",
    "\n",
    "#     if args.parse_mode == args.generate_mode == False:\n",
    "#         print(\"ERROR: -i / --parse and -o / --generate cannot both be false\", file=sys.stderr)\n",
    "#         argparser.print_help(sys.stderr)\n",
    "#         sys.exit(2)\n",
    "\n",
    "#     if not args.grammar_files:\n",
    "#         print(\"ERROR: grammar files required\", file=sys.stderr)\n",
    "#         argparser.print_help(sys.stderr)\n",
    "#         sys.exit(2)\n",
    "\n",
    "#     if not args.allowed_words_file:\n",
    "#         print(\"ERROR: allowed words filename required\", file=sys.stderr)\n",
    "#         argparser.print_help(sys.stderr)\n",
    "#         sys.exit(2)\n",
    "\n",
    "#     if args.verbose:\n",
    "#         print(\"#verbose level: {}\".format(args.verbose), file=sys.stderr)\n",
    "#         print(\"#mode: {}\".format(\"parse\" if args.parse_mode else \"generate\"), file=sys.stderr)\n",
    "#         print(\"#grammar: {}\".format(\" \".join(args.grammar_files)), file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar_files = ['treebank_grammar_new.txt','newvocab.txt']\n",
    "startsym = 'TOP'\n",
    "allowed_words_file = '../../allowed_words.txt'\n",
    "verbose = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "#reading grammar file: treebank_grammar_new.txt\n",
      "#reading grammar file: newvocab.txt\n",
      "#Ignored cycle , -> ,\n",
      "#Ignored cycle . -> .\n",
      "#Ignored cycle : -> :\n"
     ]
    }
   ],
   "source": [
    "# gram = Pcfg(args.grammar_files, args.startsym, args.allowed_words_file, args.verbose)\n",
    "gram = Pcfg(grammar_files, startsym, allowed_words_file, verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = PcfgGenerator(gram,verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gen.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "certain words that A Providence confuse praised any wound which drinks until the new word An use To needs Arthur I nine seven problem European ' than huge these lives to built return To answer will To saved What frequently another names does used using went assist all some An use English the science important arm work needs warning Sir Two To attend make lying A Have to Those town By that everything\n"
     ]
    }
   ],
   "source": [
    "# print(gram)\n",
    "\n",
    "generate_mode = 1\n",
    "num_sentences = 1\n",
    "\n",
    "# if args.generate_mode:\n",
    "if generate_mode:\n",
    "    gen = PcfgGenerator(gram, verbose=0)\n",
    "#     for _ in range(args.num_sentences):\n",
    "    for _ in range(num_sentences):\n",
    "        \n",
    "        print(\" \".join(gen.generate()))\n",
    "\n",
    "# if args.parse_mode:\n",
    "#     parser = CkyParse(gram, args.verbose, args.use_prior, args.use_pruning, args.beam, args.unseen_file)\n",
    "#     parser.parse_stream(sys.stdin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
