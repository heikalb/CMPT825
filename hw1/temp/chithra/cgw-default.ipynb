{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to generate Vocab.gr\n",
    "##### 1. From nltk.book read Monty Python and the Holy Grail and allowed_words.txt\n",
    "##### 2. Word Tokenize\n",
    "##### 3. Identify parts of speech using RegexpTagger,UnigramTagger, BigramTagger and PerceptronTagger\n",
    "##### 4. Write it to Vocab.gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 999)\n",
    "pd.set_option('display.max_columns', 999)\n",
    "from nltk.book import *\n",
    "from nltk.corpus import webtext\n",
    "data = webtext.raw('grail.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('allowed_words.txt', 'r') as myfile:\n",
    "    actual_data=myfile.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = nltk.word_tokenize(data)\n",
    "actual_text = nltk.word_tokenize(actual_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sents = brown.tagged_sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sents = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [\n",
    "    (r'.*ing$', 'VBG'),               # gerunds\n",
    "    (r'.*ed$', 'VBD'),                # simple past\n",
    "    (r'.*es$', 'VBZ'),                # 3rd singular present\n",
    "    (r'.*ould$', 'MD'),               # modals\n",
    "    (r'.*\\'s$', 'NN$'),               # possessive nouns\n",
    "    (r'.*s$', 'NNS'),                 # plural nouns\n",
    "    (r'^-?[0-9]+(.[0-9]+)?$', 'CD'),  # cardinal numbers\n",
    "    (r'(The|the|A|a|An|an)$', 'AT'),   # articles \n",
    "    (r'.*able$', 'JJ'),                # adjectives \n",
    "    (r'.*ness$', 'NN'),                # nouns formed from adjectives\n",
    "    (r'(.*ly|Alright|alright)$', 'RB'),                  # adverbs\n",
    "    (r'(He|he|She|she|It|it|I|me|Me|You|you)$', 'PRP'), # pronouns\n",
    "    (r'(His|his|Her|her|Its|its)$', 'PRP$'),    # possesive\n",
    "    (r'(my|Your|your|Yours|yours)$', 'PRP$'),   # possesive\n",
    "    (r'(on|On|in|In|at|At|since|Since)$', 'IN'),# time prepopsitions\n",
    "    (r'(for|For|ago|Ago|before|Before)$', 'IN'),# time prepopsitions\n",
    "    (r'(till|Till|until|Until)$', 'IN'),        # time prepopsitions\n",
    "    (r'(by|By|beside|Beside)$', 'IN'),          # space prepopsitions\n",
    "    (r'(under|Under|below|Below)$', 'IN'),      # space prepopsitions\n",
    "    (r'(over|Over|above|Above)$', 'IN'),        # space prepopsitions\n",
    "    (r'(across|Across|through|Through)$', 'IN'),# space prepopsitions\n",
    "    (r'(into|Into|towards|Towards)$', 'IN'),    # space prepopsitions\n",
    "    (r'(onto|Onto|from|From)$', 'IN'),          # space prepopsitions    \n",
    "    # WARNING : Put the default value in the end\n",
    "    (r'.*', 'NN')                      # nouns (default)\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag.perceptron import PerceptronTagger\n",
    "pct_tag = PerceptronTagger(load=False)\n",
    "model = pct_tag.train(train_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = nltk.RegexpTagger(patterns)\n",
    "t1 = nltk.UnigramTagger(train_sents,model=model, backoff=t0)\n",
    "t2 = nltk.UnigramTagger(train_sents, backoff=t1)\n",
    "t3 = nltk.BigramTagger(train_sents,cutoff=2, backoff=t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_list = sorted(set(t3.tag(text)))\n",
    "actual_pos_list = sorted(set(t3.tag(actual_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding default weight of 1. Change it later\n",
    "pos_list = [(1,) + pos for pos in pos_list]\n",
    "actual_pos_list = [(1,) + pos for pos in actual_pos_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(pos_list, columns=['Weight', 'Word', 'POS'])\n",
    "df = df[['Weight','POS','Word']]\n",
    "\n",
    "\n",
    "actual_df = pd.DataFrame(actual_pos_list, columns=['Weight', 'Word', 'POS'])\n",
    "actual_df = actual_df[['Weight','POS','Word']]\n",
    "\n",
    "new_df = pd.concat([df,actual_df],axis=0)\n",
    "new_df = new_df.drop_duplicates()\n",
    "new_df = new_df.sort_values('POS')\n",
    "#Uncomment this if Vocab.gr is not generated\n",
    "new_df.to_csv('Vocab.gr',sep='\\t',index=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def get_rules(tree):\n",
    "    # Get subtrees of the tree\n",
    "    subtrees = tree.subtrees()\n",
    "    rules = []\n",
    "\n",
    "    # For each subtree get the rule that generates it's immediate child node\n",
    "    for subtree in subtrees:\n",
    "        children = [ch for ch in subtree]\n",
    "\n",
    "        # Skip nodes that lead to leaves\n",
    "        if len(children) > 0 and not type(children[0]) == str:\n",
    "            # get labels of child nodes\n",
    "            child_labels = [fix_label(ch.label()) for ch in children]\n",
    "\n",
    "            # create rule string\n",
    "            curr_rule = '\\t'.join([subtree.label()] + child_labels)\n",
    "\n",
    "            # Exclude rules with unneeded nodes\n",
    "            if not exclude_rule(curr_rule):\n",
    "                rules.append(curr_rule)\n",
    "\n",
    "    return rules\n",
    "\n",
    "\n",
    "def fix_label(label):\n",
    "    if not re.search(r'[a-zA-Z]', label):\n",
    "        return 'Punc-' + label\n",
    "\n",
    "    return label\n",
    "\n",
    "\n",
    "def exclude_rule(rule):\n",
    "    for n in unneeded_nodes:\n",
    "        if n in rule:\n",
    "            return True\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "unneeded_nodes = ['$', '-NONE-']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get trees for Penn Treebank\n",
    "trees_by_file = [treebank.parsed_sents(id) for id in treebank.fileids()]\n",
    "trees = [tree for files in trees_by_file for tree in files]\n",
    "\n",
    "# converting trees into Chomsky Normal Forms\n",
    "for i in range(len(trees)):\n",
    "    tree = trees[i]\n",
    "    tree.chomsky_normal_form()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = [rule for tree in trees for rule in get_rules(tree)]\n",
    "\n",
    "rule_counts = Counter(rules)\n",
    "rules = list(set(rules))\n",
    "rules.sort()\n",
    "\n",
    "# Save rules\n",
    "with open('S2.gr', 'w') as f:\n",
    "    f.write('# Rules found in Penn Treebank\\n# Number of rules: {0}\\n'.format(len(rules)))\n",
    "\n",
    "    for r in rules:\n",
    "        f.write('{0}\\t\\t{1}\\n'.format(rule_counts[r], r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
