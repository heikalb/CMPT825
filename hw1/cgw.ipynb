{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework: Competitive Grammar Writing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group northernwolfpack \n",
    "\n",
    "* Chithra Bhat, cbhat\n",
    "* Gustavo Felhberg, gfelhber\n",
    "* Heikal Badrulhisham, hbadrulh\n",
    "* Helen Zhang, hyz1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to develop the grammar files and the vocabulary, we decided to try two different approaches and, in the end, chose the one with the best results.\n",
    "\n",
    "### Penn Treebank (best results)\n",
    "\n",
    "The first approach was to extract grammatical rules from syntactic trees in a sample of the Penn Treebank, available in the python package NLTK (nltk.corpus.treebank). \n",
    "\n",
    "Initially, we extracted all the grammar trees available in the treebank corpus and converted them to Chomsky Normal Form using the NLTK method 'tree.chomsky_normal_form()'. Then, for each of the trees, we extracted the respective rules using a method get_rules(tree) developed by our group. Using a similar approach, we extracted the rules existent in the 'devset.trees' file and combined these rules with those extracted from the Penn Treebank. After merging the rules, we counted their frequency and saved the grammar file in a PCFG format. The source code with the methods used to extract the grammar is available in the files 'get_treebank_grammar.py' and 'get_devset_rule.py'.\n",
    "\n",
    "The vocabulary was built using the following corpora: brown, treebank, conll2000 and nps_chat. After filtering out the words not allowed, we obtained their part of speech and savedthem in the Vocab.gr file. The source code with the methods to extract the vocab is available in the file 'build_vocab.py'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main files:\n",
    "\n",
    "* get_treebank_grammar.py:\n",
    "\n",
    "    * extract the trees from Penn Treebank\n",
    "    * convert to CNF\n",
    "    * extract the rules from trees\n",
    "    * add rules extracted from the devset trees\n",
    "    * counts the rules frequency \n",
    "    * add misc rules\n",
    "    * save rules into grammar file\n",
    "    \n",
    "    \n",
    "* get_devset_rules.py:\n",
    "\n",
    "    * extract trees from devset.trees file\n",
    "    * convert to CNF\n",
    "    * extract rules from trees\n",
    "    * save 'devset_rules.txt' file\n",
    "\n",
    "\n",
    "* build_vocab.py:\n",
    "\n",
    "    * extracts the tagged words from corpora treebank, conll2000, brown and nps chat\n",
    "    * filter only allowed words\n",
    "    * extract the parts of speach of each (POS) word\n",
    "    * count the occurrence of the words and respective POS\n",
    "    * read vocab included manually in file 'manual_vocab.txt'\n",
    "    * save the final vocabulary file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stanford Parser\n",
    "\n",
    "The second approach was to use the Stanford Parser (https://nlp.stanford.edu/software/lex-parser.shtml) to extract the vocabulary and the grammatical rules. We used the content of the book Monty Python and the Holy Grail available in the NLTK library, tokenized the sentences, and used the parser to extract the rules. The source code with the methods to extract the grammar and vocabulary is available in /hw1/stanford_parser/ folder. The main file stanford_parser.ipynb is a Jupyter Notebook with the descriptions of the steps in order to generate the grammar and vocabulary using this method.\n",
    "\n",
    "#### Main files:\n",
    "\n",
    "* stanford_parser.ipynb:\n",
    "\n",
    "    * load the Monty Python and the Holy Grail corpus\n",
    "    * extract the vocabulary and calculate the frequency, respecting the allowed words restriction\n",
    "    * extract the sentences from the corpus\n",
    "    * reads the Monty Python sentences and generate a pandas dataframe \n",
    "    * clean the strings (remove parts like 'SCENE 1:', 'KING ARTHUR:', '[wind]', '[clop clop clop]', etc.)\n",
    "    * extract trees, convert to CNF and extract the rules\n",
    "    * calculate the frequency of the rules\n",
    "    * save into a grammar file    \n",
    "    \n",
    "    \n",
    "* parse_sentence.ipynb:\n",
    "\n",
    "    * parse individual sentences and output the vocab with POS and the grammar rules as in the image below:\n",
    "\n",
    "<img src=\"./stanford_parser/parse_sentence.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised taggers:\n",
    "\n",
    "In order to evaluate the taggers on the treebank corpus, we first started with unsupervised taggers. \n",
    "* nltk.DefaultTagger: We applied the most frequent tag, i.e. the NN tag to the default tagger. \n",
    "* nltk.RegexpTagger: The regular expression tagger assigns tags to tokens on the basis of matching patterns. Regular expression tagger by itself is limited to very common language properties; therefore it is able to tag\n",
    "only few sentences of the whole corpus correctly.\n",
    "    \n",
    "### Supervised taggers:\n",
    "\n",
    "* nltk.UnigramTagger: Unigram taggers are based on a simple statistical algorithm: It assigns the tags with the most probable tag by calculating the frequencies of each token i.e., for each token, assign the tag that is most likely for that particular token.\n",
    "* nltk.BigramTagger: An n-gram tagger is a generalization of a unigram tagger whose context is the current word together with the part-of-speech tags of the n-1 preceding tokens.  \n",
    "\n",
    "### Combining taggers:\n",
    "One way to address the trade-off between accuracy and coverage is to use the more accurate algorithms when we can, but to fall back on algorithms with wider coverage when necessary. We used the content of the book Monty Python and the Holy Grail available in the NLTK library, tokenized the sentences, and used the parser to extract the rules.\n",
    "\n",
    "* Approach\n",
    "    * Tag the tokens with the bigram tagger.\n",
    "    * If the bigram tagger is unable to find a tag for the token, try the unigram tagger.\n",
    "    * If the unigram tagger is also unable to find a tag, use a regex/default tagger.\n",
    "    \n",
    "#### Main files:\n",
    "\n",
    "* temp/chithra/cgw-default.ipynb:\n",
    "\n",
    "    * Load the Monty Python and the Holy Grail text file\n",
    "    * Train the model(combining taggers) on Brown corpus tagged sentences\n",
    "    * Provide Monty Python corpus sentences as test set for the model \n",
    "    * For every word, identify POS and save that to a Pandas dataframe\n",
    "    * Save this dataframe as vocabulary file.\n",
    "    * Read Monty Python sentences\n",
    "    * Extract trees, convert to CNF and extract the rules\n",
    "    * Save the results to a grammar file  \n",
    "    \n",
    "However, the grammar generated by the Penn treebank approach had better results, so we decided to use that as our official result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
